{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Fill in task 1a image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2b)\n",
    "\n",
    "![](task2b_binary_train_loss.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2c)\n",
    "\n",
    "![](task2b_binary_train_accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2d)\n",
    "Early stopping kicks in after 33 epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2e)\n",
    "It happens because shuffling makes it less likely that the validation set will contain only examples from the same class. Thus the model's performance on the validation set will be more representative of how it would perform on new data. The opposite would be overfitting to the validation set.\n",
    "\n",
    "![](task2e_train_accuracy_shuffle_difference.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3b)\n",
    "<img src=\"task3b_softmax_train_loss.png?modified=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3c)\n",
    "<img src=\"task3b_softmax_train_accuracy.png?modified=1\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3d)\n",
    "There are signs of overfitting, namely that the accuracy is much higher on the training set than it is on the validation set. This is a sign that the model is unable to generalize well to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Fill in image of hand-written notes which are easy to read, or latex equations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "The model with regularization has less noisy weights because the regularization term penalizes large weights and therefore the model is encouraged to find weights that are smaller in magnitude which could mean that the weights are less noisy. This is because smaller weights tend to produce less change in the model's output for a given input, which can make the model's predictions more stable and less sensitive to small variations in the input.\n",
    "\n",
    "<img src=\"task4b_softmax_weight_with_without_reg.png?modified=1\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4c)\n",
    "\n",
    "<img src=\"task4c_diff_lambdas.png?modified=2\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4d)\n",
    "Because a single layer NN is already quite simple and probably not able to generalize very well, adding a further restriction will make it even harder. A model this simple needs all the wiggle room it can get in terms of finding good weights to be able to learn a good representation of the data, therefore adding large regularization will in this case be too restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4e)\n",
    "We can see that the magnitude of the weights decrease as the regularization term increases. This makes sense, because regularization is supposed to reduce model complexity by penalizing large weights. \n",
    "\n",
    "<img src=\"task4e_l2reg_norms.png?modified=\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
